{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    accidents_original = pd.read_csv(\"datasets/dftRoadSafety_Accidents_2016.csv\",\n",
    "                                     #index_col = \"Accident_Index\",\n",
    "                                     low_memory = False)\n",
    "    \n",
    "    accidents_original = accidents_original.reindex(np.random.permutation(accidents_original.shape[0]))\n",
    "    print(\"dataset shape : \" + str(accidents_original.shape))\n",
    "    #print(accidents_original.head())\n",
    "    return accidents_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(accidents_original):\n",
    "    features = accidents_original.filter(\n",
    "        ['Day_of_Week', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', 'Light_Conditions',\n",
    "         'Weather_Conditions', 'Road_Surface_Conditions'], axis=1)\n",
    "    \n",
    "    features['Speed_limit'] = features['Speed_limit']/10\n",
    "\n",
    "    cols_to_transform = ['Day_of_Week', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control',\n",
    "                         'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions']\n",
    "\n",
    "    features = pd.get_dummies(features, columns = cols_to_transform)\n",
    "    return features\n",
    "\n",
    "def preprocess_targets(accidents_original):\n",
    "    targets = accidents_original.filter(['Accident_Severity'], axis=1)\n",
    "    targets = targets - 1\n",
    "    print(\"unique valus in target : \" + str(targets['Accident_Severity'].unique()))\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(features, targets, batch_size=1000, shuffle=None, num_epochs=None):\n",
    "    features = {key: np.array(value) for key, value in dict(features).items()}\n",
    "\n",
    "    ds = Dataset.from_tensor_slices((features, targets))\n",
    "    \n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100000)\n",
    "\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test(accidents_features, accidents_targets, test_size):\n",
    "    features_train, fetures_test, targets_train, targets_test = train_test_split(accidents_features, accidents_targets, test_size=test_size)\n",
    "    print(\"features-training shape : \" + str(features_train.shape))\n",
    "    print(\"targets-training shape : \" + str(targets_train.shape))\n",
    "    print(\"features-test shape : \" + str(fetures_test.shape))\n",
    "    print(\"targets-test shape : \" + str(targets_test.shape))\n",
    "    return features_train, fetures_test, targets_train, targets_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_column(features):\n",
    "    return set([tf.feature_column.numeric_column(f) for f in features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_optimizer(optimizer, learning_rate):\n",
    "    if optimizer == \"GD\":\n",
    "        print(\"initilizing GD Optimizer\")\n",
    "        my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    elif optimizer == \"ADAGRAD\":\n",
    "        print(\"initializing ADAGRAD\")\n",
    "        my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    elif optimizer == \"ADAM\":\n",
    "        print(\"initializing ADAM\")\n",
    "        my_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    return tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "\n",
    "def construct_model(features_train, hidden_units, model, my_optimizer):\n",
    "    feature_columns = construct_feature_column(features_train)\n",
    "    if model == \"linear\":\n",
    "        print(\"initializing linear classfier\")\n",
    "        classifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n",
    "                                                   n_classes=3,\n",
    "                                                   optimizer=my_optimizer)\n",
    "    elif model == \"DNN\":\n",
    "        print(\"initializing DNN classifier\")\n",
    "        classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                                n_classes=3,\n",
    "                                                hidden_units=hidden_units,\n",
    "                                                optimizer=my_optimizer)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def plot_log_loss(training_log_losses, validation_log_losses):\n",
    "    plt.title(\"LogLoss vs Iterations\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_log_losses, label=\"training\")\n",
    "    plt.plot(validation_log_losses, label=\"validation\")\n",
    "    \n",
    "    \n",
    "def train_model(training_features, training_targets,\n",
    "                validation_features, validation_targets,\n",
    "                model=\"linear\", optimizer=\"GD\", hidden_units=None,\n",
    "                learning_rate=None, steps=None, batch_size=None):\n",
    "    iterations = 10\n",
    "    steps_per_iteration = steps / iterations\n",
    "    print(\"steps per iteration : \" + str(steps_per_iteration))\n",
    "    \n",
    "    my_optimizer = get_optimizer(optimizer, learning_rate)\n",
    "\n",
    "    classifier = construct_model(training_features, hidden_units, model, my_optimizer)\n",
    "\n",
    "    training_input_fn = lambda : my_input_fn(training_features, training_targets['Accident_Severity'], \n",
    "                                             batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    predict_training_input_fn = lambda  : my_input_fn(training_features, training_targets['Accident_Severity'], \n",
    "                                                      num_epochs=1, shuffle=False)\n",
    "    \n",
    "    predict_validation_input_fn = lambda : my_input_fn(validation_features, validation_targets['Accident_Severity'], \n",
    "                                                       num_epochs=1, shuffle=False)\n",
    "    \n",
    "    print(\"training model...\")\n",
    "    training_log_losses = []\n",
    "    validation_log_losses = []\n",
    "    for iteration in range(iterations):\n",
    "        classifier.train(input_fn=training_input_fn, steps=steps_per_iteration)\n",
    "        \n",
    "        training_predictions = classifier.predict(input_fn=predict_training_input_fn)\n",
    "        training_predictions = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_predictions)\n",
    "        \n",
    "        # validation_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "        # validation_predictions = np.array([item['probabilities'] for item in validation_predictions])\n",
    "        # validation_log_loss = metrics.log_loss(validation_targets, validation_predictions)\n",
    "\n",
    "        print(\"  iteration %02d : %0.2f\" % (iteration, training_log_loss))\n",
    "        # # Add the loss metrics from this period to our list.\n",
    "        # training_log_losses.append(training_log_loss)\n",
    "        # validation_log_losses.append(validation_log_loss)\n",
    "\n",
    "    print(\"training model finished\")\n",
    "    plot_log_loss(training_log_losses, validation_log_losses)\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape : (136621, 32)\nunique valus in target : [2 1 0]\nfeatures : ['Day_of_Week_1', 'Day_of_Week_2', 'Day_of_Week_3', 'Day_of_Week_4', 'Day_of_Week_5', 'Day_of_Week_6', 'Day_of_Week_7', 'Road_Type_-1', 'Road_Type_1', 'Road_Type_2', 'Road_Type_3', 'Road_Type_6', 'Road_Type_7', 'Road_Type_9', 'Speed_limit_2.0', 'Speed_limit_3.0', 'Speed_limit_4.0', 'Speed_limit_5.0', 'Speed_limit_6.0', 'Speed_limit_7.0', 'Junction_Detail_-1', 'Junction_Detail_0', 'Junction_Detail_1', 'Junction_Detail_2', 'Junction_Detail_3', 'Junction_Detail_5', 'Junction_Detail_6', 'Junction_Detail_7', 'Junction_Detail_8', 'Junction_Detail_9', 'Junction_Control_-1', 'Junction_Control_0', 'Junction_Control_1', 'Junction_Control_2', 'Junction_Control_3', 'Junction_Control_4', 'Light_Conditions_-1', 'Light_Conditions_1', 'Light_Conditions_4', 'Light_Conditions_5', 'Light_Conditions_6', 'Light_Conditions_7', 'Weather_Conditions_-1', 'Weather_Conditions_1', 'Weather_Conditions_2', 'Weather_Conditions_3', 'Weather_Conditions_4', 'Weather_Conditions_5', 'Weather_Conditions_6', 'Weather_Conditions_7', 'Weather_Conditions_8', 'Weather_Conditions_9', 'Road_Surface_Conditions_-1', 'Road_Surface_Conditions_1', 'Road_Surface_Conditions_2', 'Road_Surface_Conditions_3', 'Road_Surface_Conditions_4', 'Road_Surface_Conditions_5']\ntarget : ['Accident_Severity']\nfeatures-training shape : (109296, 58)\ntargets-training shape : (109296, 1)\nfeatures-test shape : (27325, 58)\ntargets-test shape : (27325, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#entry point\n",
    "accidents_original = load_data()\n",
    "\n",
    "accidents_features = preprocess_features(accidents_original)\n",
    "\n",
    "accidents_targets = preprocess_targets(accidents_original)\n",
    "\n",
    "print(\"features : \" + str(list(accidents_features.columns.values)))\n",
    "print(\"target : \" + str(list(accidents_targets.columns.values)))\n",
    "\n",
    "training_features, validation_features, training_targets, validation_targets = \\\n",
    "    split_into_train_test(accidents_features, accidents_targets, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Day_of_Week_1  Day_of_Week_2  Day_of_Week_3  Day_of_Week_4  \\\ncount  136621.000000  136621.000000  136621.000000  136621.000000   \nmean        0.109968       0.141091       0.147152       0.152766   \nstd         0.312852       0.348117       0.354258       0.359763   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n       Day_of_Week_5  Day_of_Week_6  Day_of_Week_7   Road_Type_-1  \\\ncount  136621.000000  136621.000000  136621.000000  136621.000000   \nmean        0.153337       0.165267       0.130419       0.000007   \nstd         0.360313       0.371423       0.336765       0.002705   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       1.000000       1.000000       1.000000   \n\n         Road_Type_1    Road_Type_2            ...              \\\ncount  136621.000000  136621.000000            ...               \nmean        0.064888       0.022815            ...               \nstd         0.246328       0.149314            ...               \nmin         0.000000       0.000000            ...               \n25%         0.000000       0.000000            ...               \n50%         0.000000       0.000000            ...               \n75%         0.000000       0.000000            ...               \nmax         1.000000       1.000000            ...               \n\n       Weather_Conditions_6  Weather_Conditions_7  Weather_Conditions_8  \\\ncount         136621.000000         136621.000000         136621.000000   \nmean               0.000578              0.005453              0.016425   \nstd                0.024040              0.073643              0.127104   \nmin                0.000000              0.000000              0.000000   \n25%                0.000000              0.000000              0.000000   \n50%                0.000000              0.000000              0.000000   \n75%                0.000000              0.000000              0.000000   \nmax                1.000000              1.000000              1.000000   \n\n       Weather_Conditions_9  Road_Surface_Conditions_-1  \\\ncount         136621.000000               136621.000000   \nmean               0.027909                    0.005629   \nstd                0.164714                    0.074814   \nmin                0.000000                    0.000000   \n25%                0.000000                    0.000000   \n50%                0.000000                    0.000000   \n75%                0.000000                    0.000000   \nmax                1.000000                    1.000000   \n\n       Road_Surface_Conditions_1  Road_Surface_Conditions_2  \\\ncount               136621.00000              136621.000000   \nmean                     0.72614                   0.250474   \nstd                      0.44594                   0.433288   \nmin                      0.00000                   0.000000   \n25%                      0.00000                   0.000000   \n50%                      1.00000                   0.000000   \n75%                      1.00000                   1.000000   \nmax                      1.00000                   1.000000   \n\n       Road_Surface_Conditions_3  Road_Surface_Conditions_4  \\\ncount              136621.000000              136621.000000   \nmean                    0.001925                   0.014698   \nstd                     0.043833                   0.120340   \nmin                     0.000000                   0.000000   \n25%                     0.000000                   0.000000   \n50%                     0.000000                   0.000000   \n75%                     0.000000                   0.000000   \nmax                     1.000000                   1.000000   \n\n       Road_Surface_Conditions_5  \ncount              136621.000000  \nmean                    0.001135  \nstd                     0.033664  \nmin                     0.000000  \n25%                     0.000000  \n50%                     0.000000  \n75%                     0.000000  \nmax                     1.000000  \n\n[8 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "print(accidents_features.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per iteration : 100.0\ninitializing ADAM\ninitializing linear classfier\ntraining model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 00 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 01 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 02 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 03 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 04 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 05 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 06 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 07 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 08 : 0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  iteration 09 : 0.49\ntraining model finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_classifier = train_model(training_features, training_targets,\n",
    "                                validation_features, validation_targets,\n",
    "                                model=\"linear\", hidden_units= [20,10], optimizer=\"ADAM\",\n",
    "                                learning_rate=0.01, steps=1000, batch_size=1000)\n",
    "\n",
    "#predict_measure(linear_classifier, fetures_test, targets_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8277402, 'average_loss': 0.4958493, 'loss': 483.89578, 'global_step': 1000}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-73f1328a6ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mevaluation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_validation_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'auc'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "predict_validation_input_fn = lambda : my_input_fn(validation_features, validation_targets['Accident_Severity'],\n",
    "                                                   num_epochs=1, shuffle=False)\n",
    "\n",
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "print(evaluation_metrics)\n",
    "#print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8277402, 'average_loss': 0.49768254, 'loss': 485.68484, 'global_step': 1000}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d7787da1c57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_validation_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'auc'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "dnn_classifier = train_model(training_features, training_targets,\n",
    "                                validation_features, validation_targets,\n",
    "                                model=\"DNN\", hidden_units= [20,10], optimizer=\"ADAM\",\n",
    "                                learning_rate=0.01, steps=1000, batch_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8277402, 'average_loss': 0.49768254, 'loss': 485.68484, 'global_step': 1000}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d7787da1c57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_validation_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on the validation set: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'auc'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "predict_validation_input_fn = lambda : my_input_fn(validation_features, validation_targets['Accident_Severity'],\n",
    "                                                   num_epochs=1, shuffle=False)\n",
    "\n",
    "evaluation_metrics = dnn_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "print(evaluation_metrics)\n",
    "print(\"AUC on the validation set: %0.2f\" % evaluation_metrics['auc'])\n",
    "print(\"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
